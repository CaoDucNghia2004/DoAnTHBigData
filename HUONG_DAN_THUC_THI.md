# ğŸš€ HÆ¯á»šNG DáºªN THá»°C THI PIPELINE CIFAKE DEEPFAKE DETECTION

## ğŸ“‹ Tá»•ng quan Pipeline

```
BÆ°á»›c 1: Upload áº£nh lÃªn HDFS
BÆ°á»›c 2: TrÃ­ch xuáº¥t Ä‘áº·c trÆ°ng (MobileNetV2)
BÆ°á»›c 3: Train Random Forest + Test + ÄÃ¡nh giÃ¡
```

---

## ğŸ”§ BÆ¯á»šC 0: KHá»I Äá»˜NG Há»† THá»NG

### 0.1. Khá»Ÿi Ä‘á»™ng táº¥t cáº£ Docker containers

```bash
docker-compose up -d
```

### 0.2. Kiá»ƒm tra containers Ä‘ang cháº¡y

```bash
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
```

**Káº¿t quáº£ mong Ä‘á»£i:** 5 containers Ä‘ang cháº¡y

-   namenode (port 9870)
-   datanode
-   spark-master (port 7077, 8080)
-   spark-worker (port 8081)
-   spark-history-server (port 18080)

### 0.3. Táº¡o thÆ° má»¥c trÃªn HDFS

```bash
docker exec namenode hdfs dfs -mkdir -p /data/cifake
docker exec namenode hdfs dfs -mkdir -p /processed
docker exec namenode hdfs dfs -mkdir -p /results
docker exec namenode hdfs dfs -mkdir -p /models
docker exec namenode hdfs dfs -mkdir -p /spark-logs
```

---

## ğŸ“¤ BÆ¯á»šC 1: UPLOAD áº¢NH LÃŠN HDFS (Ingestion)

### 1.1. Copy script upload vÃ o spark-master

```bash
docker cp spark-jobs/upload_to_hdfs.py spark-master:/opt/spark/work-dir/
```

### 1.2. Cháº¡y upload áº£nh lÃªn HDFS

```bash
docker exec spark-master python /opt/spark/work-dir/upload_to_hdfs.py
```

**Thá»i gian:** ~10-15 phÃºt (120,000 áº£nh)

### 1.3. Kiá»ƒm tra áº£nh Ä‘Ã£ upload

```bash
docker exec namenode hdfs dfs -ls /data/cifake/
docker exec namenode hdfs dfs -count /data/cifake/
```

**Káº¿t quáº£ mong Ä‘á»£i:** 120,000 files trÃªn HDFS

---

## ğŸ” BÆ¯á»šC 2: TRÃCH XUáº¤T Äáº¶C TRÆ¯NG (Feature Extraction)

### 2.1. Copy script feature extraction vÃ o spark-master

```bash
docker cp spark-jobs/cifake_feature_extraction.py spark-master:/opt/spark/work-dir/
```

### 2.2. Cháº¡y trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng vá»›i MobileNetV2

```bash
docker exec spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --driver-memory 4g \
    --executor-memory 4g \
    --executor-cores 2 \
    --conf spark.executor.memoryOverhead=2g \
    /opt/spark/work-dir/cifake_feature_extraction.py
```

**Thá»i gian:** ~60-90 phÃºt (xá»­ lÃ½ 120,000 áº£nh qua MobileNetV2)

### 2.3. Kiá»ƒm tra features Ä‘Ã£ trÃ­ch xuáº¥t

```bash
docker exec namenode hdfs dfs -ls /processed/cifake_features/
```

**Káº¿t quáº£:** File parquet chá»©a 120,000 vectors (má»—i vector 1280 chiá»u)

---

## ğŸ¤– BÆ¯á»šC 3: TRAIN RANDOM FOREST + TEST + ÄÃNH GIÃ

### 3.1. Copy script classifier vÃ o spark-master

```bash
docker cp spark-jobs/cifake_classifier.py spark-master:/opt/spark/work-dir/
```

### 3.2. Cháº¡y train vÃ  Ä‘Ã¡nh giÃ¡ model

```bash
docker exec spark-master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --deploy-mode client \
    --driver-memory 2g \
    --executor-memory 4g \
    --executor-cores 4 \
    --conf spark.executor.memoryOverhead=1g \
    --conf spark.sql.shuffle.partitions=20 \
    /opt/spark/work-dir/cifake_classifier.py
```

**Thá»i gian:** ~30-45 phÃºt

-   Train: 100,000 áº£nh
-   Test: 20,000 áº£nh

### 3.3. Kiá»ƒm tra káº¿t quáº£

```bash
docker exec namenode hdfs dfs -ls /results/
docker exec namenode hdfs dfs -ls /models/
```

**Káº¿t quáº£:**

-   `/results/cifake_metrics` - Accuracy, Precision, Recall, F1
-   `/results/cifake_predictions` - Dá»± Ä‘oÃ¡n chi tiáº¿t
-   `/models/cifake_randomforest` - Model Ä‘Ã£ train

---

## ğŸ“Š BÆ¯á»šC 4: XEM Káº¾T QUáº¢ (Business Insight)

### 4.1. Copy script Ä‘á»c metrics

```bash
docker cp spark-jobs/read_metrics.py spark-master:/opt/spark/work-dir/
```

### 4.2. Xem káº¿t quáº£ Ä‘Ã¡nh giÃ¡ model

```bash
docker exec spark-master /opt/spark/bin/spark-submit \
    --master local[*] \
    --driver-memory 1g \
    /opt/spark/work-dir/read_metrics.py
```

**Káº¿t quáº£ mong Ä‘á»£i:**

```
+-------------+--------+-----------+--------+----------+
|model        |accuracy|precision  |recall  |f1_score  |
+-------------+--------+-----------+--------+----------+
|random_forest|0.8776  |0.8778     |0.8776  |0.8775    |
+-------------+--------+-----------+--------+----------+
```

---

## ğŸŒ BÆ¯á»šC 5: XEM SPARK HISTORY SERVER (Báº±ng chá»©ng)

### 5.1. Má»Ÿ trÃ¬nh duyá»‡t truy cáº­p

```
http://localhost:18080
```

### 5.2. Chá»¥p mÃ n hÃ¬nh cÃ¡c má»¥c sau Ä‘á»ƒ bÃ¡o cÃ¡o:

-   Danh sÃ¡ch Applications Ä‘Ã£ cháº¡y
-   Stages/Tasks cháº¡y song song
-   Timeline cá»§a job

---

## ğŸ›‘ Dá»ªNG Há»† THá»NG

### Dá»«ng táº¥t cáº£ containers

```bash
docker-compose down
```

### Dá»«ng vÃ  xÃ³a dá»¯ liá»‡u (cáº©n tháº­n!)

```bash
docker-compose down -v
```

---

## ğŸ“ Cáº¤U TRÃšC THÆ¯ Má»¤C HDFS

```
/data/cifake/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ FAKE/     (50,000 áº£nh)
â”‚   â””â”€â”€ REAL/     (50,000 áº£nh)
â””â”€â”€ test/
    â”œâ”€â”€ FAKE/     (10,000 áº£nh)
    â””â”€â”€ REAL/     (10,000 áº£nh)

/processed/
â””â”€â”€ cifake_features/    (Parquet - 120,000 vectors)

/results/
â”œâ”€â”€ cifake_metrics/     (Accuracy, Precision, Recall)
â””â”€â”€ cifake_predictions/ (Dá»± Ä‘oÃ¡n chi tiáº¿t)

/models/
â””â”€â”€ cifake_randomforest/ (Model Ä‘Ã£ train)

/spark-logs/            (Event logs cho History Server)
```

---

## âš ï¸ Xá»¬ LÃ Lá»–I THÆ¯á»œNG Gáº¶P

### Lá»—i 1: Container khÃ´ng khá»Ÿi Ä‘á»™ng

```bash
docker-compose down
docker-compose up -d
```

### Lá»—i 2: HDFS khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c

```bash
docker restart namenode datanode
# Äá»£i 30 giÃ¢y
docker exec namenode hdfs dfs -ls /
```

### Lá»—i 3: Spark job bá»‹ kill do thiáº¿u memory

-   Giáº£m `--executor-memory` xuá»‘ng 2g
-   Giáº£m `--driver-memory` xuá»‘ng 1g

### Lá»—i 4: History Server khÃ´ng hiá»ƒn thá»‹

```bash
docker restart spark-history-server
# Äá»£i 10 giÃ¢y rá»“i truy cáº­p http://localhost:18080
```

---

## ğŸ“ˆ Káº¾T QUáº¢ Äáº T ÄÆ¯á»¢C

| Metric        | GiÃ¡ trá»‹ |
| ------------- | ------- |
| **Accuracy**  | 87.76%  |
| **Precision** | 87.78%  |
| **Recall**    | 87.76%  |
| **F1-Score**  | 87.75%  |

**Káº¿t luáº­n:** Model MobileNetV2 + Random Forest cÃ³ thá»ƒ phÃ¡t hiá»‡n áº£nh Deepfake vá»›i Ä‘á»™ chÃ­nh xÃ¡c ~88%

docker exec spark-master /opt/spark/bin/spark-submit \
 --master local[*] \
 /opt/spark/work-dir/spark-jobs/verify_pipeline.py

docker exec spark-master /opt/spark/bin/spark-submit --master local[*] /opt/spark/work-dir/spark-jobs/verify_pipeline.py
